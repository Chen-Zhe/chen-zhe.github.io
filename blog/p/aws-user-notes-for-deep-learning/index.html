<!DOCTYPE html>
<html lang="en-us">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='The meanings of some of the AWS terminologies and how to use these technologies effectively and efficiently for deep learning. Updated on 2022-05-16.'><title>AWS User Notes for Deep Learning</title>

<link rel='canonical' href='https://chen-zhe.github.io/blog/p/aws-user-notes-for-deep-learning/'>

<link rel="stylesheet" href="/blog/scss/style.min.775dbd4fd34fda61c5273b4bc3415f7c9666414fb6c40aab164a7ded4397da98.css"><meta property='og:title' content='AWS User Notes for Deep Learning'>
<meta property='og:description' content='The meanings of some of the AWS terminologies and how to use these technologies effectively and efficiently for deep learning. Updated on 2022-05-16.'>
<meta property='og:url' content='https://chen-zhe.github.io/blog/p/aws-user-notes-for-deep-learning/'>
<meta property='og:site_name' content='大白鲸笔记本'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:published_time' content='2021-01-27T00:00:00&#43;00:00'/><meta property='article:modified_time' content='2021-01-27T00:00:00&#43;00:00'/>
<meta name="twitter:title" content="AWS User Notes for Deep Learning">
<meta name="twitter:description" content="The meanings of some of the AWS terminologies and how to use these technologies effectively and efficiently for deep learning. Updated on 2022-05-16.">
    <link rel="shortcut icon" href="https://chen-zhe.github.io/favicon.png" />

<link href="https://chen-zhe.github.io/assets/fonts/lato/lato_family.css" rel="stylesheet" type="text/css">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-D1QVYGCXRR"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-D1QVYGCXRR', { 'anonymize_ip': false });
}
</script>

    </head>
    <body class="
    article-page has-toc
">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex 
    
        extended
    
">
    
        <div id="article-toolbar">
            <a href="/blog" class="back-home">
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="15 6 9 12 15 18" />
</svg>



                <span>Back</span>
            </a>
        </div>
    
<main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/blog/categories/cloud/" >
                Cloud
            </a>
        
            <a href="/blog/categories/deep-learning/" >
                Deep Learning
            </a>
        
    </header>
    

    <h2 class="article-title">
        <a href="/blog/p/aws-user-notes-for-deep-learning/">AWS User Notes for Deep Learning</a>
    </h2>

    
    <h3 class="article-subtitle">
        The meanings of some of the AWS terminologies and how to use these technologies effectively and efficiently for deep learning. Updated on 2022-05-16.
    </h3>
    

    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Jan 27, 2021</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    14 minute read
                </time>
            </div>
        
    </footer>
    
</div>
</header>

    <section class="article-content">
    <h2 id="introduction">Introduction</h2>
<p>Recently I&rsquo;ve used AWS to train machine learning / deep learning models and run inferences,
and here are my notes and observations about the platform for this purpose.</p>
<p>Overall speaking, AWS is a complex platform with a rather steep learning curve
if I were to take advantage of services other than EC2 itself.
Here are my notes for services that I&rsquo;ve used throughout the fast-paced learning journey
and hopefully they can be of help to others.</p>
<p>There are other platforms that offer competitive pricing for deep learning applications
such as <a class="link" href="https://vast.ai/"  target="_blank" rel="noopener"
    >vast.ai</a> and <a class="link" href="https://datacrunch.io/"  target="_blank" rel="noopener"
    >DataCrunch.io</a>,
but the basics of using remote machines for the purpose of deep learning should be
transferrable.</p>
<h3 id="recitation-videos-youtube">Recitation Videos (YouTube)</h3>
<p>This is the recitation video series that I&rsquo;ve made for
the Fall 2021 version of 11-785 Introduction to Deep Learning at Carnegie Mellon University.
It includes hands-on setups of a GPU-backed EC2 spot instance and a Conda+PyTorch environment using Deep Learning Base AMI (Ubuntu 18.04).</p>
<p>The following blog post is the companion writeup of this video series, though this post can also be referred to independently.</p>
<p style="text-align:center">
<iframe width="80%" height="400" src="https://www.youtube.com/embed/videoseries?list=PLjd84g1GiDh2HcLTo02dcpBKg2Wtmt3AD" title="YouTube video player" frameborder="0" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</p>
<h2 id="glossary-with-section-link">Glossary with Section Link</h2>
<ul>
<li><a class="link" href="#ec2" >EC2</a>: Elastic Compute Cloud</li>
<li><a class="link" href="#ssh" >SSH</a>: Secure Shell</li>
<li><a class="link" href="#ami" >AMI</a>: Amazon Machine Image</li>
<li><a class="link" href="#ebs" >EBS</a>: Elastic Block Storage</li>
<li><a class="link" href="#efs" >EFS</a>: Elastic File System</li>
<li><a class="link" href="#s3" >S3</a>: Simple Storage Service</li>
<li><a class="link" href="#access-s3-bucket-in-ec2" >IAM</a>: Identity and Access Management</li>
</ul>
<h2 id="tldr-my-workflow">TL;DR. My Workflow</h2>
<h3 id="1-configure-custom-deep-learning-environment">1. Configure Custom Deep Learning Environment</h3>
<p>Install miniconda3 on an EC2 instance using AWS Deep Learning Base AMI (Ubuntu 18.04)
and installed all necessary packages such as PyTorch and pandas:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># Miniconda with Python 3.8</span>
</span></span><span class="line"><span class="cl">wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
</span></span><span class="line"><span class="cl">chmod u+x Miniconda3-latest-Linux-x86_64.sh <span class="c1"># make it executable</span>
</span></span><span class="line"><span class="cl">./Miniconda3-latest-Linux-x86_64.sh <span class="c1"># start installer</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Check https://pytorch.org/get-started/locally/ for the latest install command</span>
</span></span><span class="line"><span class="cl">conda install pytorch torchvision torchaudio <span class="nv">cudatoolkit</span><span class="o">=</span>11.3 -c pytorch
</span></span><span class="line"><span class="cl">conda install pandas scikit-learn jupyterlab matplotlib tqdm seaborn
</span></span><span class="line"><span class="cl">pip install kaggle
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">conda clean -a <span class="c1"># remove downloaded package zips</span>
</span></span></code></pre></div><p>While installing Jupyter Lab, Conda will automatically install its dependencies,
such as <code>ipython</code>.</p>
<h3 id="2-configure-kaggle-and-jupyter-lab-access">2. Configure Kaggle and Jupyter Lab Access</h3>
<p>Store your Kaggle key (<code>kaggle.json</code>) in the <code>.kaggle</code> folder under <code>/home/ubuntu/</code>.</p>
<h4 id="jupyter-lab-access-method-1-external-access">Jupyter Lab Access Method 1: External Access</h4>
<p>For Jupyter Lab, follow the <a class="link" href="https://jupyter-notebook.readthedocs.io/en/stable/public_server.html"  target="_blank" rel="noopener"
    >docs</a> to configuring external access. But the following shows a simpler version:</p>
<p>Generate hashed Jupyter Lab password by running the following piece of Python code:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">notebook.auth</span> <span class="kn">import</span> <span class="n">passwd</span>
</span></span><span class="line"><span class="cl"><span class="n">my_password</span> <span class="o">=</span> <span class="s2">&#34;password&#34;</span> <span class="c1"># set your desired password here</span>
</span></span><span class="line"><span class="cl"><span class="n">hashed_password</span> <span class="o">=</span> <span class="n">passwd</span><span class="p">(</span><span class="n">passphrase</span><span class="o">=</span><span class="n">my_password</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s1">&#39;sha256&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">hashed_password</span><span class="p">)</span> <span class="c1"># copy the hashed password</span>
</span></span></code></pre></div><p>Then create a new file <code>jupyter_server_config.py</code> under <code>.jupyter</code> folder
in the home directory with the following content:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">c</span><span class="o">.</span><span class="n">ServerApp</span><span class="o">.</span><span class="n">ip</span> <span class="o">=</span> <span class="s1">&#39;*&#39;</span> <span class="c1"># bind to any network interface</span>
</span></span><span class="line"><span class="cl"><span class="n">c</span><span class="o">.</span><span class="n">ServerApp</span><span class="o">.</span><span class="n">password</span> <span class="o">=</span> <span class="sa">u</span><span class="s1">&#39;sha256:bcd259ccf...&lt;your hashed password here&gt;&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">c</span><span class="o">.</span><span class="n">ServerApp</span><span class="o">.</span><span class="n">open_browser</span> <span class="o">=</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl"><span class="n">c</span><span class="o">.</span><span class="n">ServerApp</span><span class="o">.</span><span class="n">port</span> <span class="o">=</span> <span class="mi">8888</span> <span class="c1"># or any other ports you&#39;d like</span>
</span></span></code></pre></div><h4 id="jupyter-lab-access-method-2-port-forwarding">Jupyter Lab Access Method 2: Port Forwarding</h4>
<p>Alternatively, you can use SSH port forwarding with the following command running on your local computer. In this case, access <code>127.0.0.1:8889</code> or <code>localhost:8889</code> while this command is running. Here, I have changed the local forwarding port to 8889 to avoid potential port conflict with your local Jupyter.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ssh -N -L 8889:localhost:8888 -i your-aws.pem ubuntu@your-ec2-ip-address
</span></span></code></pre></div><h3 id="3-tar-the-configured-environment-and-save-to-efs">3. Tar the configured environment and save to EFS</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">tar -cf ~/efs/dl-env.tar ./miniconda3 .kaggle .ipython .jupyter .conda .bashrc
</span></span></code></pre></div><p>Note that I didn&rsquo;t use the <code>z</code> option to compress the files, as my tests showed that
due to the sheer number of files that I&rsquo;m putting into this archive,
adding compression <em>significantly</em> slows down the tar/untar process and time is
much more valuable than the cost of extra storage space required.</p>
<h3 id="4-deploy-saved-environment-in-a-new-ec2-instance">4. Deploy Saved Environment in a new EC2 instance</h3>
<p>Launch a new instance with pre-configured security group and run</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># first connect to EFS and with working directory as ~</span>
</span></span><span class="line"><span class="cl">tar -xf efs/dl-env.tar <span class="c1"># will run for ~2 minutes</span>
</span></span><span class="line"><span class="cl"><span class="nb">source</span> .bashrc
</span></span></code></pre></div><p>Voila, the conda environment is up and running!</p>
<h3 id="5-update-saved-environment">5. Update Saved Environment</h3>
<p>If you made any changes to your environment, e.g. installed new packages,
run the following command to (incrementally) update the tar</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">tar -uvf efs/dl-env.tar miniconda3/ .conda <span class="c1"># assuming environment update</span>
</span></span></code></pre></div><h2 id="region">Region</h2>
<p>AWS regions such as <em>US East (N. Virginia)</em> <code>us-east-1</code> and <em>US East (Ohio)</em> <code>us-east-2</code>
are basically their data centers located within the region. Network transit within region
is free of charge but is chargeable otherwise.</p>
<p>Each region further divide into availability zones, such as <code>us-east-2a</code>. EBS volumes
created in a specific zone can only be mounted to EC2 instances within the same zone.</p>
<p>Side note: there are ways to duplicate EBS volumes across availability zones but it seemed too
troublesome to me, so I recommend always backing up important data in a region-shared file system like EFS.</p>
<h2 id="ec2">EC2</h2>
<p>EC2 is a virtual machine service but you can only choose from their confusingly-named
presets (CPU and memory combo) as opposed to custom configurations. I assume this is
to simplify their scheduling algorithm.</p>
<h3 id="increase-limit">Increase Limit</h3>
<p>Newly registered AWS users have to first manually increase their limits/service quota
in order to launch bigger instances or use GPU-backed instances.</p>
<p>Here&rsquo;s the <a class="link" href="https://us-east-2.console.aws.amazon.com/ec2/v2/home?region=us-east-2#Limits:"  target="_blank" rel="noopener"
    >URL</a>
to AWS console&rsquo;s limit page such that you can increase it. Here are the ones you need to make request
to increase. Just request for 64 vCores on all of following:</p>
<ul>
<li>Running instances
<ul>
<li>Running On-Demand All G instances</li>
<li>Running On-Demand All P instances</li>
<li>Running On-Demand All Standard (A, C, D, H, I, M, R, T, Z) instances</li>
</ul>
</li>
<li>Requested instances
<ul>
<li>All G Spot Instance Requests</li>
<li>All P Spot Instance Requests</li>
<li>All Standard (A, C, D, H, I, M, R, T, Z) Spot Instance Requests</li>
</ul>
</li>
</ul>
<h3 id="security-group">Security Group</h3>
<p>It&rsquo;s basically like an old-school firewall that allows network access on specific ports.</p>
<p>Necessary inbound rules</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Protocol</th>
<th>Port Range</th>
<th>Source</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>SSH</td>
<td>(auto)</td>
<td>(auto)</td>
<td>0.0.0.0/0</td>
<td>Unrestricted in case your IP address changed</td>
</tr>
<tr>
<td>NFS</td>
<td>(auto)</td>
<td>(auto)</td>
<td>Security group attached to EC2 (I just use the same one)</td>
<td>Allow EFS access</td>
</tr>
<tr>
<td>Custom TCP</td>
<td>TCP</td>
<td>8888</td>
<td>0.0.0.0/0</td>
<td>Unrestricted Jupyter Lab access in case you want to access it from different IPs. Change this if you configured Jupyter Lab to use a different port. Not needed if you use the SSH port forwarding approach</td>
</tr>
</tbody>
</table>
<p>Necessary outbound rules</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Protocol</th>
<th>Port Range</th>
<th>Destination</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr>
<td>HTTP</td>
<td>(auto)</td>
<td>(auto)</td>
<td>0.0.0.0/0</td>
<td>Allow EC2 to download external data</td>
</tr>
<tr>
<td>HTTPS</td>
<td>(auto)</td>
<td>(auto)</td>
<td>0.0.0.0/0</td>
<td>Allow EC2 to download external data</td>
</tr>
<tr>
<td>SSH</td>
<td>(auto)</td>
<td>(auto)</td>
<td>0.0.0.0/0</td>
<td>Automatically added</td>
</tr>
<tr>
<td>NFS</td>
<td>(auto)</td>
<td>(auto)</td>
<td>Security group attached to EC2</td>
<td>Automatically added</td>
</tr>
</tbody>
</table>
<h3 id="type-selection">Type Selection</h3>
<p><a class="link" href="https://aws.amazon.com/ec2/instance-types/"  target="_blank" rel="noopener"
    >EC2 instance type and name list</a></p>
<p>For machine learning, compute-optimized C5 series makes the most sense due to their
higher CPU-to-memory ratio. I used <code>c5.24xlarge</code> (with 96 vCores) for tasks that can take advantage of
multiple cores.</p>
<p>As a side note, C5a instances uses AMD EPYC processors and there&rsquo;s a limited
number of them, so one of my extra-large instance using C5a was stopped due to
insufficient resource and couldn&rsquo;t be resumed, yikes!</p>
<p>For deep learning, G series is a good choice. Specifically for single GPU training:</p>
<ul>
<li><code>g4dn.xlarge</code>: 4 vCores, 16GB memory and a Tesla T4
<ul>
<li>Spot pricing: ~0.158 USD/Hour</li>
</ul>
</li>
<li><code>p3.2xlarge</code>: 8 vCores, 61GB memory and a Tesla V100
<ul>
<li>Spot pricing: ~0.918 USD/Hour</li>
</ul>
</li>
</ul>
<h4 id="using-ephemeral-drive">Using Ephemeral Drive</h4>
<p><code>g4dn</code> series comes with a ephemeral drive that can be used to store temporary data,
such as unzipped training data. Be warned that any data stored in this drive will be erased
when the instance is stopped, hence the name &ldquo;ephemeral&rdquo;. Its size varies with instance types.
For example, <code>g4dn.xlarge</code> comes with a 125GB drive and <code>g4dn.2xlarge</code> comes with a 250GB drive.</p>
<p>Ephemeral drive is usually detected by Ubuntu OS as <code>/dev/nvme1n1</code>. Follow the
<a class="link" href="#mounting-an-ebs-volume-to-ec2" >guide</a>
below on mounting EBS volumes to mount this drive.
In cases where this device name is occupied by a secondary EBS volume, it might be renamed as
<code>/dev/nvme2n1</code>.</p>
<h3 id="suspend-vs-stop-vs-terminate">Suspend vs Stop vs Terminate</h3>
<p>When suspending an EC2 instance, its memory content will be written to (probably the boot) EBS
such that the any task that are running when the instance is suspended can resume once the instance
is woken up.
As such, you need to ensure that the boot EBS volume has enough spare capacity
for storing the entire memory.
It&rsquo;s like hibernation on Windows.
However, not all AMIs supports this. The instance&rsquo;s ephemeral IP address will also change, so you&rsquo;d need to use the new IP address for SSH.</p>
<p>Stopping an EC2 instance <strong>will not</strong> remove its boot EBS volume and it can be restarted again. It&rsquo;s basically
like shutting down your computer. However, stopping and restarting will change the instance&rsquo;s
ephemeral IP address, too.</p>
<p>Terminating an EC2 instance <strong>will</strong> remove its boot EBS volume and it&rsquo;s gone forever!</p>
<h3 id="vcore-performance">vCore Performance</h3>
<p>vCores are much lower than physical CPU cores, hence <strong>parallelism is very important</strong>!
By my estimation, a vCore only runs at 50% of the speed of my laptop&rsquo;s i7-8750H core.
Make sure your <code>DataLoader</code> can run on as many vCores as possible to keep the Tesla GPU
from data starvation.</p>
<h4 id="burstable-cpu">Burstable CPU</h4>
<p>Burstable CPU is a feature of T2 series general-purpose VMs. It basically means
you&rsquo;ll be charged extra when you almost always use all cores for compute, but if
the machine is mostly idle (like a web/database server), this could be a cost-saver.</p>
<p>This is probably not suitable for training models since you want to push all
cores to the max (ideally) for the best performance. But if you are running some
data analysis task using Jupyter Notebook, this type of instance could be a good fit.</p>
<h3 id="spot-instance">Spot Instance</h3>
<p><a class="link" href="https://aws.amazon.com/ec2/spot/pricing/"  target="_blank" rel="noopener"
    >Spot Instance Pricing</a></p>
<p>Spot instances are much cheaper than regular timed instances. The only downside
is that it could be stopped by AWS at any time. But my experience shows that
it doesn&rsquo;t happen very often, at least in <code>us-east-2</code>(Ohio).</p>
<p>If you do not check the <strong>Persistent request</strong> box when launching a EC2 instance,
the one-time-request spot instance will be terminated directly when it is stopped.</p>
<p>You <strong>must</strong> cancel the request from the <a class="link" href="https://us-east-2.console.aws.amazon.com/ec2sp/v2/home?region=us-east-2#/spot"  target="_blank" rel="noopener"
    >Spot Requests</a> page when you want to terminate the instance. Otherwise,
a persistent request will relaunch the instance when you terminated it from the EC2
management console.</p>
<p>If you are getting a &ldquo;spot capacity error&rdquo; message when launching <code>g4dn.xlarge</code> spot instances, I advise waiting until nightfall at US East Coast and then try again, or try launching <code>g4dn.2xlarge</code> instead.</p>
<h3 id="ssh">SSH</h3>
<p>Any SSH client would work, but I do highly recommend <a class="link" href="https://mobaxterm.mobatek.net/"  target="_blank" rel="noopener"
    >MobaXterm</a>
for Windows users (I&rsquo;m one).</p>
<p>You would need a key pair to access the EC2 instance. This file can be generated
when launching the EC2 instance and reused. Each key can only be downloaded once
so don&rsquo;t lose it. The full command line using ssh would look like:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ssh -i /path/my-key-pair.pem user-name@my-instance-public-ip-address
</span></span></code></pre></div><p>Username would be <code>ec2-user</code> for regular Amazon AMIs and <code>ubuntu</code> for AWS Deep Learning AMI.</p>
<h3 id="ami">AMI</h3>
<p>AMI is basically a prepackaged system disk image with pre-configured environment.</p>
<p>I&rsquo;m really impressed with its boot speed, which only takes a few seconds.
I certainly feel that it&rsquo;s faster than Google Cloud Compute instances in terms of
boot speed.</p>
<p>AWS Deep Learning AMI comes with Anaconda, PyTorch and TensorFlow (with choices of versions, too)
so that you can run your code straightaway. A big time saver! However, this
beefy image also requires at least 100GB of boot volume, so EBS cost is going to be a factor
if you decide to keep the instance for some time.</p>
<p>However, AWS Deep Learning AMI does not support suspending the instance, so be sure to
write code for saving to and restoring from checkpoints, in case the spot instance
was stopped by AWS while your model has not been trained for enough epochs.</p>
<p>AWS Deep Learning Base AMI is a slimmed-down version of the AWS Deep Learning AMI.
It requires a minimum EBS disk size of 60GB and comes with necessary GPU drivers and
linear algebra packs. However, it doesn&rsquo;t come with any deep learning environment, so you
need to configure one on your own.</p>
<h3 id="monitoring">Monitoring</h3>
<p><code>htop</code> for a command-line task manager to monitor CPU usage</p>
<p><code>nvidia-smi</code> for a summary of GPU usage</p>
<h2 id="ebs">EBS</h2>
<p>EBS serves as a hard drive for EC2 instances. Each EC2 instance will have a boot
EBS volume, but you can attach additional EBS volumes to it.</p>
<p>Resizing EBS would require filesystem level operations on the instance OS, so
I would recommend allocate enough storage on the boot volume to start with.</p>
<p>By default, EBS volumes are SSD-backed <code>gp2</code>. It&rsquo;s not expensive and have pretty good performance
for my use case,
so I&rsquo;d just stick with it instead of downgrading to a HDD-backed option.</p>
<p>Note that the IOPS (I/O operations per second) of an <code>gp2</code> EBS volume is proportional
to its size (<a class="link" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html"  target="_blank" rel="noopener"
    >up to 5,334 GB</a>).
Therefore, it seems to me that it&rsquo;s better to allocate a large-enough EBS boot volume
for AMI, training data and some buffer so that you get overall better performance.</p>
<h3 id="mounting-an-ebs-volume-to-ec2">Mounting an EBS volume to EC2</h3>
<p>In case you need some temporary storage, you can create a new EBS volume and attach it
to your EC2 instance (<strong>in the same availability zone</strong>) on AWS console.</p>
<p>Once you&rsquo;ve done that, you need to attach and format the disk in the OS. For Linux,
the steps are (assume the volume is detected as <code>/dev/xvdf</code>):</p>
<ol>
<li>
<p><code>ls /dev</code> to find the new EBS volume device. I found that sometimes it has the name
of <code>xvdf</code> (last character is variable) and other times it has the name of <code>nvme1p1</code>.
You can compare the output before and after attaching the volume on UI so as
to identify the new device.</p>
</li>
<li>
<p><code>sudo mkfs -t xfs /dev/xvdf</code> to format the volume. Skip if the volume is
already formatted (e.g. it was used by another instance earlier).</p>
</li>
<li>
<p><code>sudo mkdir ~/data &amp;&amp; sudo mount /dev/xvdf ~/data</code> to mount the volume in
a new <code>~/data</code> directory.</p>
</li>
<li>
<p><code>cd ~/data &amp;&amp; sudo chmod go+rw .</code> to give read-write permissions to non-root users.</p>
</li>
</ol>
<h2 id="efs">EFS</h2>
<p>EFS is a networked filesystem that can be shared across multiple instances in the
same region. Since it&rsquo;s managed by AWS, it is dynamically sized and charged based on the amount
of data you stored in EFS. You also don&rsquo;t need to worry about availability zone,
since it will provide mounting points in all of them.</p>
<p>I find that it&rsquo;s most convenient as both a shared drive across multiple instances and a backup
location. The shared drive functionality allows me to run inference in one instance and
score the result in another. I also backup training data and scripts in EFS such that
I can terminate my instances but some time later I found that I need to retrain the model.</p>
<p>Accessing EFS will consume EC2 instance&rsquo;s network bandwidth, so I usually copy the frequently accessed
files out to the EBS volume. When copying files, it can sustain a read speed of
close to 1Gbps.</p>
<p>In terms of pricing, EFS is more expensive than EBS per GB. However, given the flexibility
and dynamic sizing, it might cost less.</p>
<h3 id="mounting-a-efs-share-to-ec2">Mounting a EFS share to EC2</h3>
<h5 id="on-aws-console">On AWS Console</h5>
<ol>
<li>
<p>Create a EFS share. This is pretty straightforward. Remember to create it
in the same region as the EC2 instances that you intend to use this share on.</p>
<ul>
<li>If you are not using your default security group, you have to add the security group to all network availability zones under the <code>network</code> tab of the EFS share management page</li>
</ul>
</li>
<li>
<p>Allows NFS port communication for the EC2 instance.</p>
<ul>
<li>Open the security group settings attached to the EC2 instance</li>
<li>Modify the inbound rule and add a rule with type <code>NFS</code>. Select source as the security group.</li>
<li>Save rules</li>
<li>Network traffic will be interrupted if and only if an existing rule is modified
and that the traffic is using the aforementioned rule</li>
</ul>
</li>
</ol>
<h5 id="on-ec2">On EC2</h5>
<ol>
<li>
<p>Install NFS client <code>nfs-utils</code> (for CentOS or REHL) or <code>nfs-common</code> (for vanilla Ubuntu).
Skip this step if the instance is using an AMI.</p>
</li>
<li>
<p><code>mkdir ~/efs</code> to make a mounting point folder.</p>
</li>
<li>
<p><code>FS_ID=file-system-id REGION=us-east-2 &amp;&amp; sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport $FS_ID.efs.$REGION.amazonaws.com:/ ~/efs</code> mounts the EFS volume
to the mounting point folder</p>
</li>
<li>
<p><code>cd ~/efs &amp;&amp; sudo chmod go+rw .</code> to give read-write permissions to non-root users.
You only need to run this command once for a new EFS share.</p>
</li>
</ol>
<h2 id="s3">S3</h2>
<p>S3 is yet another storage service for file storage. To access the files, you need to use
<code>aws</code> commands. As such, it cannot be used as a regular disk like EFS or EBS.</p>
<p>For my use case, I find S3
suitable for sharing large files, such as trained model weights, via HTTPS.
I&rsquo;ve also seen use cases that use S3 as a data lake. Apache Spark even supports reading
data directly off S3.</p>
<h3 id="access-s3-bucket-in-ec2">Access S3 bucket in EC2</h3>
<h4 id="on-aws-console-1">On AWS Console</h4>
<ol>
<li>
<p>Create an S3 bucket in the region you intended to use.</p>
</li>
<li>
<p>Create a IAM role with S3 full access</p>
<ul>
<li>In Identity and Access Management (IAM) page, click <strong>Create role</strong></li>
<li>Choose <strong>EC2</strong> as use case</li>
<li>In attach permission policies, find <strong>AmazonS3FullAccess</strong> and check it</li>
<li>Save the role and give it a name</li>
</ul>
</li>
<li>
<p>Attach the role to the intended EC2 instance</p>
<ul>
<li>In the instance list, select the instance</li>
<li>Choose <strong>Actions, Security, Modify IAM role</strong></li>
<li>Select the role you just created and choose <strong>Save</strong></li>
</ul>
</li>
</ol>
<h4 id="on-ec2-1">On EC2</h4>
<ol>
<li><code>aws s3 cp my_copied_file.ext s3://my_bucket/my_folder/my_file.ext</code> to upload
files to S3. Reverse the 2 arguments to download files from S3.</li>
</ol>
<h3 id="access-s3-objects-from-https">Access S3 objects from HTTPS</h3>
<ol>
<li>
<p>Go to the S3 console and modify its ACL read permission to be <strong>Everyone</strong>.
This is not safe but the files I&rsquo;m sharing aren&rsquo;t going to make sense for others anyway.</p>
</li>
<li>
<p>Access the file using <code>https://my-bucket.s3.us-east-2.amazonaws.com/my_folder/my_file.ext</code>.
This assume that your S3 bucket is created in <code>us-east-2</code></p>
</li>
</ol>

</section>


    <footer class="article-footer">
    

    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>


    
</article>

    

    <aside class="related-contents--wrapper">
    
    
</aside>

     
    
        
    <div class="disqus-container">
    <div id="disqus_thread"></div>
<script type="application/javascript">
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "chen-zhe" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

<style>
    .disqus-container {
        background-color: var(--card-background);
        border-radius: var(--card-border-radius);
        box-shadow: var(--shadow-l1);
        padding: var(--card-padding);
    }
</style>

<script>
    window.addEventListener('onColorSchemeChange', (e) => {
        if (typeof(DISQUS) != 'undefined' && DISQUS) {
            DISQUS.reset({
                reload: true
            });
        }
    })
</script>

    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2018 - 
        
        2023 陈哲 Chen Zhe
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.6.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer="true"
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer="true"
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css"integrity="sha256-c0uckgykQ9v5k&#43;IqViZOZKc47Jn7KQil4/MP3ySA3F8="crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css"integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE="crossorigin="anonymous"
            >

            </main>
    
        <aside class="sidebar right-sidebar sticky">
            <section class="widget archives">
                <div class="widget-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



                </div>
                <h2 class="widget-title section-title">Table of contents</h2>
                
                <div class="widget--toc">
                    <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a>
      <ul>
        <li><a href="#recitation-videos-youtube">Recitation Videos (YouTube)</a></li>
      </ul>
    </li>
    <li><a href="#glossary-with-section-link">Glossary with Section Link</a></li>
    <li><a href="#tldr-my-workflow">TL;DR. My Workflow</a>
      <ul>
        <li><a href="#1-configure-custom-deep-learning-environment">1. Configure Custom Deep Learning Environment</a></li>
        <li><a href="#2-configure-kaggle-and-jupyter-lab-access">2. Configure Kaggle and Jupyter Lab Access</a></li>
        <li><a href="#3-tar-the-configured-environment-and-save-to-efs">3. Tar the configured environment and save to EFS</a></li>
        <li><a href="#4-deploy-saved-environment-in-a-new-ec2-instance">4. Deploy Saved Environment in a new EC2 instance</a></li>
        <li><a href="#5-update-saved-environment">5. Update Saved Environment</a></li>
      </ul>
    </li>
    <li><a href="#region">Region</a></li>
    <li><a href="#ec2">EC2</a>
      <ul>
        <li><a href="#increase-limit">Increase Limit</a></li>
        <li><a href="#security-group">Security Group</a></li>
        <li><a href="#type-selection">Type Selection</a></li>
        <li><a href="#suspend-vs-stop-vs-terminate">Suspend vs Stop vs Terminate</a></li>
        <li><a href="#vcore-performance">vCore Performance</a></li>
        <li><a href="#spot-instance">Spot Instance</a></li>
        <li><a href="#ssh">SSH</a></li>
        <li><a href="#ami">AMI</a></li>
        <li><a href="#monitoring">Monitoring</a></li>
      </ul>
    </li>
    <li><a href="#ebs">EBS</a>
      <ul>
        <li><a href="#mounting-an-ebs-volume-to-ec2">Mounting an EBS volume to EC2</a></li>
      </ul>
    </li>
    <li><a href="#efs">EFS</a>
      <ul>
        <li><a href="#mounting-a-efs-share-to-ec2">Mounting a EFS share to EC2</a></li>
      </ul>
    </li>
    <li><a href="#s3">S3</a>
      <ul>
        <li><a href="#access-s3-bucket-in-ec2">Access S3 bucket in EC2</a></li>
        <li><a href="#access-s3-objects-from-https">Access S3 objects from HTTPS</a></li>
      </ul>
    </li>
  </ul>
</nav>
                </div>
            </section>
        </aside>
    

        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js"integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g="crossorigin="anonymous"
                defer="false"
                >
            </script><script type="text/javascript" src="/blog/ts/main.js" defer></script>

    </body>
</html>
