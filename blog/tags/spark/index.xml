<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>spark on 大白鲸笔记本</title>
    <link>https://chen-zhe.github.io/blog/tags/spark/</link>
    <description>Recent content in spark on 大白鲸笔记本</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Sep 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://chen-zhe.github.io/blog/tags/spark/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Apache Spark - Save Time with Less Join Operations</title>
      <link>https://chen-zhe.github.io/blog/p/apache-spark-save-time-with-less-join-operations/</link>
      <pubDate>Fri, 28 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://chen-zhe.github.io/blog/p/apache-spark-save-time-with-less-join-operations/</guid>
      <description>Not long ago, I was tasked to improve our Spark application&amp;rsquo;s runtime performance as some modules takes 7 hours or more to complete on datasets that are not considered very large. This is my conclusion and afterthoughts after countless hours staring at our codebase and YARN application tracking UI. I was able to achieve 30% ~ 80% reduction of runtime, depending on how well the module was written and the nature of operations.</description>
    </item>
    
  </channel>
</rss>
