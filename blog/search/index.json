[{"content":"Updates 2022-01-04: To quote AWS support, \u0026ldquo;Over the last few months, we have seen a rapid increase in customer demand for our high-performance GPU instances. Several AWS teams are still working around the clock to provision additional capacity for our customers and remediate this situation.\u0026rdquo; Currently it has been very difficult to launch g4dn.xlarge spot instances without getting spot capacity error. I advise wait until night at East Coast to try again and launch g4dn.2xlarge instead. Hopefully this situation gets improved before Spring semester starts.\n2022-03-12: The resource crunch situation in us-east-2 seem to be improved.\nIntroduction Recently I\u0026rsquo;ve used AWS to train machine learning / deep learning models and run inferences, and here are my notes and observations about the platform for this purpose\nOverall speaking, AWS is a complex platform with a rather steep learning curve if I were to take advantage of services other than EC2 itself.\nHere are my notes for services that I\u0026rsquo;ve used throughout the rapid-paced learning journey and hopefully they can be of help to others.\nGlossary with Section Link  EC2: Elastic Compute Cloud SSH: Secure Shell AMI: Amazon Machine Image EBS: Elastic Block Storage EFS: Elastic File System S3: Simple Storage Service IAM: Identity and Access Management  TL;DR. My Workflow 1. Configure Custom Deep Learning Environment Install miniconda3 on an EC2 instance using AWS Deep Learning Base AMI (Ubuntu 18.04) and installed all necessary packages such as PyTorch and pandas:\n# Miniconda with Python 3.8 wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh chmod u+x Miniconda3-latest-Linux-x86_64.sh # make it executable ./Miniconda3-latest-Linux-x86_64.sh # start installer # Check https://pytorch.org/get-started/locally/ for the latest install command conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch conda install pandas scikit-learn jupyterlab matplotlib tqdm seaborn pip install kaggle conda clean -a # remove downloaded package zips While installing Jupyter Lab, Conda will automatically install its dependencies, such as ipython.\n2. Configure Kaggle and Jupyter Lab Access Store your Kaggle key (kaggle.json) in the .kaggle folder under /home/ubuntu/.\nJupyter Lab Access Method 1: External Access For Jupyter Lab, follow the docs to configuring external access. But the following shows a simpler version:\nGenerate hashed Jupyter Lab password by running the following piece of Python code:\nfrom notebook.auth import passwd my_password = \u0026#34;password\u0026#34; # set your desired password here hashed_password = passwd(passphrase=my_password, algorithm=\u0026#39;sha256\u0026#39;) print(hashed_password) # copy the hashed password Then create a new file jupyter_server_config.py under .jupyter folder in the home directory with the following content:\nc.ServerApp.ip = \u0026#39;*\u0026#39; # bind to any network interface c.ServerApp.password = u\u0026#39;sha256:bcd259ccf...\u0026lt;your hashed password here\u0026gt;\u0026#39; c.ServerApp.open_browser = False c.ServerApp.port = 8888 # or any other ports you\u0026#39;d like Jupyter Lab Access Method 2: Port Forwarding Alternatively, you can use SSH port forwarding with the following command running on your local computer. In this case, access 127.0.0.1:8889 or localhost:8889 while this command is running. Here, I have changed the local forwarding port to 8889 to avoid potential port conflict with your local Jupyter.\nssh -N -L 8889:localhost:8888 -i your-aws.pem ubuntu@your-ec2-ip-address 3. Tar the configured environment and save to EFS tar -cf ~/efs/dl-env.tar ./miniconda3 .kaggle .ipython .jupyter .conda .bashrc Note that I didn\u0026rsquo;t use the z option to compress the files, as my tests showed that due to the sheer number of files that I\u0026rsquo;m putting into this archive, adding compression significantly slows down the tar/untar process and time is much more valuable than the cost of extra storage space required.\n4. Deploy Saved Environment in a new EC2 instance Launch a new instance with pre-configured security group and run\n# first connect to EFS and with working directory as ~ tar -xf efs/dl-env.tar # will run for ~2 minutes source .bashrc Voila, the conda environment is up and running!\n5. Update Saved Environment If you made any changes to your environment, e.g. installed new packages, run the following command to (incrementally) update the tar\ntar -uvf efs/dl-env.tar miniconda3/ .conda # assuming environment update Region AWS regions such as US East (N. Virginia) us-east-1 and US East (Ohio) us-east-2 are basically their data centers located within the region. Network transit within region is free of charge but is chargeable otherwise.\nEach region further divide into availability zones, such as us-east-2a. EBS volumes created in a specific zone can only be mounted to EC2 instances within the same zone.\nSide note: there are ways to duplicate EBS volumes across availability zones but it seemed too troublesome to me, so always backup important data in a shared file system like EFS.\nEC2 EC2 is a virtual machine service but you can only choose from their confusingly-named presets (CPU and memory combo) as opposed to custom configurations. I assume this is to simplify their scheduling algorithm.\nIncrease Limit Newly registered AWS users have to first manually increase their limits/service quota in order to launch bigger instances or use GPU-backed instances.\nHere\u0026rsquo;s the URL to AWS console\u0026rsquo;s limit page such that you can increase it. Here are the ones you need to make request to increase. Just request for 64 vCores on all of following:\n Running instances  Running On-Demand All G instances Running On-Demand All P instances Running On-Demand All Standard (A, C, D, H, I, M, R, T, Z) instances   Requested instances  All G Spot Instance Requests All P Spot Instance Requests All Standard (A, C, D, H, I, M, R, T, Z) Spot Instance Requests    Security Group It\u0026rsquo;s basically like an old-school firewall that allows network access on specific ports.\nNecessary inbound rules\n   Type Protocol Port Range Source Reason     SSH (auto) (auto) 0.0.0.0/0 Unrestricted in case your IP address changed   NFS (auto) (auto) Security group attached to EC2 (I just use the same one) Allow EFS access   Custom TCP TCP 8888 0.0.0.0/0 Unrestricted Jupyter Lab access in case you want to access it from different IPs. Change this if you configured Jupyter Lab to use a different port. Not needed if you use the SSH port forwarding approach    Necessary outbound rules\n   Type Protocol Port Range Destination Reason     HTTP (auto) (auto) 0.0.0.0/0 Allow EC2 to download external data   HTTPS (auto) (auto) 0.0.0.0/0 Allow EC2 to download external data   SSH (auto) (auto) 0.0.0.0/0 Automatically added   NFS (auto) (auto) Security group attached to EC2 Automatically added    Type Selection EC2 instance type and name list\nFor machine learning, compute-optimized C5 series makes the most sense due to their higher CPU-to-memory ratio. I used c5.24xlarge (with 96 vCores) for tasks that can take advantage of multiple cores.\nAs a side note, C5a instances uses AMD EPYC processors and there\u0026rsquo;s a limited number of them, so one of my extra-large instance using C5a was stopped due to insufficient resource and couldn\u0026rsquo;t be resumed, yikes!\nFor deep learning, G series is a good choice. Specifically for single GPU training:\n g4dn.xlarge: 4 vCores, 16GB memory and a Tesla T4  Spot pricing: ~0.158 USD/Hour   p3.2xlarge: 8 vCores, 61GB memory and a Tesla V100  Spot pricing: ~0.918 USD/Hour    Using Ephemeral Drive g4dn series comes with a ephemeral drive that can be used to store temporary data, such as unzipped training data. Be warned that any data stored in this drive will be erased when the instance is stopped, hence the name \u0026ldquo;ephemeral\u0026rdquo;. Its size varies with instance types. For example, g4dn.xlarge comes with a 125GB drive and g4dn.2xlarge comes with a 250GB drive.\nEphemeral drive is usually detected by Ubuntu OS as /dev/nvme1n1. Follow the guide below on mounting EBS volumes to mount this drive. In cases where this device name is occupied by a secondary EBS volume, it might be renamed as /dev/nvme2n1.\nSuspend vs Stop vs Terminate When suspending an EC2 instance, its memory content will be written to (probably the boot) EBS such that the any task that are running when the instance is suspended can resume once the instance is woken up. As such, you need to ensure that the boot EBS volume has enough spare capacity for storing the entire memory. It\u0026rsquo;s like hibernation on Windows. However, not all AMIs supports this. The instance\u0026rsquo;s ephemeral IP address will also change, so you\u0026rsquo;d need to use the new IP address for SSH.\nStopping an EC2 instance will not remove its boot EBS volume and it can be restarted again. It\u0026rsquo;s basically like shutting down your computer. However, stopping and restarting will change the instance\u0026rsquo;s ephemeral IP address, too.\nTerminating an EC2 instance will remove its boot EBS volume and it\u0026rsquo;s gone forever!\nvCore Performance vCores are much lower than physical CPU cores, hence parallelism is very important! By my estimation, a vCore only runs at 50% of the speed of my laptop\u0026rsquo;s i7-8750H core. Make sure your DataLoader can run on as many vCores as possible to keep the Tesla GPU from data starvation.\nBurstable CPU Burstable CPU is a feature of T2 series general-purpose VMs. It basically means you\u0026rsquo;ll be charged extra when you almost always use all cores for compute, but if the machine is mostly idle (like a web/database server), this could be a cost-saver.\nThis is probably not suitable for training models since you want to push all cores to the max (ideally) for the best performance. But if you are running some data analysis task using Jupyter Notebook, this type of instance could be a good fit.\nSpot Instance Spot Instance Pricing\nSpot instances are much cheaper than regular timed instances. The only downside is that it could be stopped by AWS at any time. But my experience shows that it doesn\u0026rsquo;t happen very often, at least in us-east-2(Ohio).\nIf you do not check the Persistent request box when launching a EC2 instance, the one-time-request spot instance will be terminated directly when it is stopped.\nYou must cancel the request from the Spot Requests page when you want to terminate the instance. Otherwise, a persistent request will relaunch the instance when you terminated it from the EC2 management console.\nSSH Any SSH client would work, but I do highly recommend MobaXterm for Windows users (I\u0026rsquo;m one).\nYou would need a key pair to access the EC2 instance. This file can be generated when launching the EC2 instance and reused. Each key can only be downloaded once so don\u0026rsquo;t lose it. The full command line using ssh would look like:\nssh -i /path/my-key-pair.pem user-name@my-instance-public-ip-address Username would be ec2-user for regular Amazon AMIs and ubuntu for AWS Deep Learning AMI.\nAMI AMI is basically a prepackaged system disk image with pre-configured environment.\nI\u0026rsquo;m really impressed with its boot speed, which only takes a few seconds. I certainly feel that it\u0026rsquo;s faster than Google Cloud Compute instances in terms of boot speed.\nAWS Deep Learning AMI comes with Anaconda, PyTorch and TensorFlow (with choices of versions, too) so that you can run your code straightaway. A big time saver! However, this beefy image also requires at least 100GB of boot volume, so EBS cost is going to be a factor if you decide to keep the instance for some time.\nHowever, AWS Deep Learning AMI does not support suspending the instance, so be sure to write code for saving to and restoring from checkpoints, in case the spot instance was stopped by AWS while your model has not been trained for enough epochs.\nAWS Deep Learning Base AMI is a slimmed-down version of the AWS Deep Learning AMI. It requires a minimum EBS disk size of 60GB and comes with necessary GPU drivers and linear algebra packs. However, it doesn\u0026rsquo;t come with any deep learning environment, so you need to configure one on your own.\nMonitoring htop for a command-line task manager to monitor CPU usage\nnvidia-smi for a summary of GPU usage\nEBS EBS serves as a hard drive for EC2 instances. Each EC2 instance will have a boot EBS volume, but you can attach additional EBS volumes to it.\nResizing EBS would require filesystem level operations on the instance OS, so I would recommend allocate enough storage on the boot volume to start with.\nBy default, EBS volumes are SSD-backed gp2. It\u0026rsquo;s not expensive and have pretty good performance for my use case, so I\u0026rsquo;d just stick with it instead of downgrading to a HDD-backed option.\nNote that the IOPS (I/O operations per second) of an gp2 EBS volume is proportional to its size (up to 5,334 GB). Therefore, it seems to me that it\u0026rsquo;s better to allocate a large-enough EBS boot volume for AMI, training data and some buffer so that you get overall better performance.\nMounting an EBS volume to EC2 In case you need some temporary storage, you can create a new EBS volume and attach it to your EC2 instance (in the same availability zone) on AWS console.\nOnce you\u0026rsquo;ve done that, you need to attach and format the disk in the OS. For Linux, the steps are (assume the volume is detected as /dev/xvdf):\n  ls /dev to find the new EBS volume device. I found that sometimes it has the name of xvdf (last character is variable) and other times it has the name of nvme1p1. You can compare the output before and after attaching the volume on UI so as to identify the new device.\n  sudo mkfs -t xfs /dev/xvdf to format the volume. Skip if the volume is already formatted (e.g. it was used by another instance earlier).\n  sudo mkdir ~/data \u0026amp;\u0026amp; sudo mount /dev/xvdf ~/data to mount the volume in a new ~/data directory.\n  cd ~/data \u0026amp;\u0026amp; sudo chmod go+rw . to give read-write permissions to non-root users.\n  EFS EFS is a networked filesystem that can be shared across multiple instances in the same region. Since it\u0026rsquo;s managed by AWS, it is dynamically sized and charged based on the amount of data you stored in EFS. You also don\u0026rsquo;t need to worry about availability zone, since it will provide mounting points in all of them.\nI find that it\u0026rsquo;s most convenient as both a shared drive across multiple instances and a backup location. The shared drive functionality allows me to run inference in one instance and score the result in another. I also backup training data and scripts in EFS such that I can terminate my instances but some time later I found that I need to retrain the model.\nAccessing EFS will consume EC2 instance\u0026rsquo;s network bandwidth, so I usually copy the frequently accessed files out to the EBS volume. When copying files, it can sustain a read speed of close to 1Gbps.\nIn terms of pricing, EFS is more expensive than EBS per GB. However, given the flexibility and dynamic sizing, it might cost less.\nMounting a EFS share to EC2 On AWS Console   Create a EFS share. This is pretty straightforward. Remember to create it in the same region as the EC2 instances that you intend to use this share on.\n If you are not using your default security group, you have to add the security group to all network availability zones under the network tab of the EFS share management page    Allows NFS port communication for the EC2 instance.\n Open the security group settings attached to the EC2 instance Modify the inbound rule and add a rule with type NFS. Select source as the security group. Save rules Network traffic will be interrupted if and only if an existing rule is modified and that the traffic is using the aforementioned rule    On EC2   Install NFS client nfs-utils (for CentOS or REHL) or nfs-common (for vanilla Ubuntu). Skip this step if the instance is using an AMI.\n  mkdir ~/efs to make a mounting point folder.\n  FS_ID=file-system-id REGION=us-east-2 \u0026amp;\u0026amp; sudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport $FS_ID.efs.$REGION.amazonaws.com:/ ~/efs mounts the EFS volume to the mounting point folder\n  cd ~/efs \u0026amp;\u0026amp; sudo chmod go+rw . to give read-write permissions to non-root users. You only need to run this command once for a new EFS share.\n  S3 S3 is yet another storage service for file storage. To access the files, you need to use aws commands. As such, it cannot be used as a regular disk like EFS or EBS.\nFor my use case, I find S3 suitable for sharing large files, such as trained model weights, via HTTPS. I\u0026rsquo;ve also seen use cases that use S3 as a data lake. Apache Spark even supports reading data directly off S3.\nAccess S3 bucket in EC2 On AWS Console   Create an S3 bucket in the region you intended to use.\n  Create a IAM role with S3 full access\n In Identity and Access Management (IAM) page, click Create role Choose EC2 as use case In attach permission policies, find AmazonS3FullAccess and check it Save the role and give it a name    Attach the role to the intended EC2 instance\n In the instance list, select the instance Choose Actions, Security, Modify IAM role Select the role you just created and choose Save    On EC2  aws s3 cp my_copied_file.ext s3://my_bucket/my_folder/my_file.ext to upload files to S3. Reverse the 2 arguments to download files from S3.  Access S3 objects from HTTPS   Go to the S3 console and modify its ACL read permission to be Everyone. This is not safe but the files I\u0026rsquo;m sharing aren\u0026rsquo;t going to make sense for others anyway.\n  Access the file using https://my-bucket.s3.us-east-2.amazonaws.com/my_folder/my_file.ext. This assume that your S3 bucket is created in us-east-2\n  ","date":"2021-01-27T00:00:00Z","permalink":"https://chen-zhe.github.io/blog/p/aws-user-notes-for-deep-learning/","title":"AWS User Notes for Deep Learning"},{"content":"TL;DR\nTo pose myself as a good software engineer candidate, I need to:\n Demonstrate passion to learn Provide more utility to the job position as a senior engineer Keep my resume short and relevant Tailor my resume for each position that I apply to Be mentally prepared for a project with relevancy to the position I\u0026rsquo;m applying to Not forget about the basics of computing (data structure and algorithms) Demonstrate Passion for coding  Background In the past few weeks, I participated in 3 interview sessions to hire more software engineers for developing the Spark ETL pipeline (using Scala) of a product. The new hire would join the Singapore development center that I am part of, which is akin to a boutique: it is not large, but everyone in it is proficient in writing production-ready code (we are developing a software product after all). Thus, the new hire is implicitly expected to have a certain level of coding skills.\nHowever, finding a candidate with skills matching our requirements (knows how to use and troubleshoot Spark, knows how to code in Scala) exactly could be difficult, so we wouldn\u0026rsquo;t mind tutoring a fresh grad, as long as he or she is driven to learn and can pick up new knowledge quickly. As it turns out:\n Passion to learn is extremely important in maintaining competitiveness\n Because if a candidate don\u0026rsquo;t have highly relevant skills that the job requires and none of the other candidates do too, the interviewers fall back to use passion to learn as the sole yardstick besides coding skills.\nSo here\u0026rsquo;s the important question that I\u0026rsquo;m trying to reason in this article: if we can only choose a single candidate, who should we hire?\nFirst disclaimer: I know we have multiple vacancies and I can only provide inputs to my manager, who will ultimately make the hiring decision. However, the question in the title serves as a good thought experiment and ultimately inspired me to write it here to serve as a reminder to myself, and hopefully not being a waste of time to anyone who read it.\nSecond disclaimer: this article is completely based on my personal experience and does not represent my company at all.\nA resume gives the first impressions Judging by their resumes, all candidates have relevant experience in the Data Engineering field, and all of them claim to know Spark (at least PySpark). In terms of industry experience, candidate A has more than 6 years, candidate B has less than 4 years, and candidate C has a few month (a final year student). The instinctive answer for me at this point is to say: hire B or C, simply because their salary requirements are definitely lower. This instinct seems inhumane at first, but when I gave it another thought, the fully learning point came to me:\n If a senior candidate cannot provide more utility to the company, the hiring manager will probably choose a junior candidate instead\n In this context, I feel that the word utility would refer to one of these:\n coding and troubleshooting experience in Spark proficiency in Scala  Notice that I used the word hiring manager instead of interviewer here, because an interviewer like me should only focus on the technical competencies of a candidate.\nThe gist of this point is that to ensure that my senior self can still be competitive in the job market, I cannot be complacent at all. I have to keep re-learning in order to stay on top of the trend, e.g. what are the new features in Spark 2.4 and Spark 3.0, and at the same time, I have to keep learning other promising technologies, too, e.g. React and Redux for asynchronous UI development (I have little experience in these advanced JavaScript-based languages, but who knows what will replace them in a few years, like how React sort of replaced AngularJS in the mainstream)\nAs for the resume itself, I realized this when I tried to read a 10-page word document about a candidate, possibly provided by a headhunter:\n A long resume is harder to scroll till the end, so an interviewer may not read beyond 1~2 pages To quote a career coach from my alma mater, \u0026ldquo;a fresh grad should keep his resume within 1 page. As he gain more experience, the resume can be extended to 2 pages\u0026rdquo;\n The headhunter \u0026ldquo;helpfully\u0026rdquo; put a long table of summary highlighting the candidate\u0026rsquo;s skill set, but it doesn\u0026rsquo;t feel very useful to me, mainly because it listed a very diverse and sometimes inconsistent (e.g. Spark, Livy, Java co-currency, multi-threading, Option) range of technologies. Frankly speaking though, it could help to pass the HR test.\nI continued to scroll down until the candidate\u0026rsquo;s resume, and quickly read it through. I find that I paid more attention to the first few pages than the rest. This particular candidate spent more than a page at the end to describe the projects he has done, and I merely read the project titles and then stopped reading. The project titles seem to duplicate his working experience and the description is in plain text, not bullet points.\nBut how to keep the resume short when I have so many things to talk about? To quote a career coach:\n Tailor my resume to each position that I\u0026rsquo;m applying to\n Since I know I\u0026rsquo;m helping to hire a data engineer focusing on Spark, I naturally skipped any projects using Python and TensorFlow, or experience developing iOS apps. So if I were to apply for a job again, I\u0026rsquo;ll just shorten the description to the experiences that doesn\u0026rsquo;t fully match the job description. If I become more senior, I\u0026rsquo;ll probably shorten or remove the extracurricular activities section of my resume.\nIn all, I find that resume is only a conversation starter rather than a decision influencer, so keeping the resume short and easily readable is more important than its layout design, unless of course, I\u0026rsquo;m interviewing for a UI/UX position.\nAny interview can be technical We have a coding component in our interview process to assess a candidate\u0026rsquo;s ability to solve a practical challenge with code, but don\u0026rsquo;t let your guard down during non-coding portion of the interview! I only attended the non-coding portion of the interview process and it feels equally technical. This portion mainly assesses what this candidate has done in his past and usually starts with one of the recent projects that the candidate has worked on, so:\n be mentally prepared for a project with relevancy to the position I\u0026rsquo;m applying to\n Both candidate A and B talked about a project in their current company, while candidate C talked about a pet project of his. In essence, it\u0026rsquo;s a conversation starter, so as long as the project has a certain degree of complexity (in our case, it should involve a data pipeline instead of merely loading the data into Pandas and running XGBoost on top of it) and I\u0026rsquo;m very familiar with it, the nature of the project (work or personal) doesn\u0026rsquo;t matter.\nThis means at first, interviewers will ask me to illustrate the design of the said project and shoot out design consideration questions based on that. But if I\u0026rsquo;m familiar with the project, this should be relatively easy.\nThen, when it\u0026rsquo;s my turn to ask questions, I came up with the following on the fly:\nTo candidate A, I asked:\n Which operation will cause a full shuffle, df.coalesce or df.repartition Performance benefit of DataFrame over RDD  These questions are specifically targeted at Spark because he indicated that he has experience with Spark on his resume as well as while describing his project. But inability to provide a satisfactory answer is not the end, it simply means that the interviewer will have to fall back to the lowest common yardstick to compare the candidates\nTo candidate B and C, I asked\n Element lookup time complexity of List and Set Describe Merge Sort and Quick Sort and their time complexity  Because they don\u0026rsquo;t know much about Spark (one of them haven\u0026rsquo;t even used Java before), I opted to ask these simple data structure and algorithm questions that test one\u0026rsquo;s basic computer science knowledge. As I\u0026rsquo;ve previously mentioned, we fall back to use the basics as the yardstick, hence:\n A software engineer shall not forget about the basics of computing (data structure and algorithms)\n Remembering how to write a lowest common ancestor function on a binary tree won\u0026rsquo;t be necessary, but the basics of computing shall not be forgotten.\nThe final decision that surprises no one Ultimately, based on the interviews, I concluded that all 3 candidates are on similar levels. So how do I make the choice if I were to choose only 1 of them? One character of candidate C made him stand out:\n Passion for coding\n Throughout his interview, he exudes passion for his pet project and actively engages with us interviewers. That ultimately made the difference.\nThis decision may not come as a surprise. After all, candidate C is probably the cheapest to hire, too. But beyond salary, the rationale behind this decision still provides enough food for thought to myself, and hopefully to some of the fellow software engineers, too.\n","date":"2020-03-15T00:00:00Z","permalink":"https://chen-zhe.github.io/blog/p/how-to-get-hired-as-a-software-engineer-a-note-to-myself-after-3-interviews-as-an-interviewer/","title":"How to get hired as a software engineer? A note to myself after 3 interviews as an interviewer"},{"content":"Presentation video at IEEE WCNC 2020\n\r\r\rWhen I was with DataSpark, I was part of a product team building a telco-specific solution that optimizes a telco operator\u0026rsquo;s spending on cell site upgrades to maximize return on investment (ROI). Requested by one of our clients, I designed, implemented and deployed this algorithm to identify cell sector clusters that share user load. Discovery of the sharing capability reduces the need to upgrade cell sites and hence saves upgrade cost. After deploying this feature, I decided to author a paper about this algorithm, together with my manager.\n","date":"2019-10-05T00:00:00Z","permalink":"https://chen-zhe.github.io/blog/p/manuscript-cell-sector-clustering/","title":"Manuscript: Cell Sector Clustering"},{"content":"GraphX is a powerful component in Spark that allows graph-based programming and manipulation at very large scale. The only drawback is that it run on RDD and not DataFrame as I\u0026rsquo;m used to in Spark 2.2. Although a Spark package called GraphFrames is available to extend GraphX\u0026rsquo;s power to DataFrame, I recon it\u0026rsquo;s still easier to learn to construct a graph using GraphX and RDD than importing the package in Spark.\nGraphX requires me to manually assign a VertexId (which is essentially Scala/Java type Long) to each vertex, which can be done through Spark DataFrame\u0026rsquo;s monotonically_increasing_id() function. This function provides unique id but doesn\u0026rsquo;t guarantee continuity (unlike SQL database\u0026rsquo;s auto-incremental primary key). However, since this id relies on partition number, it is NOT stable! The solution, though, is easy: cache the DataFrame with id (or saving it to disk) will make it persistent across operations (e.g. join).\nThe following piece of code converts a graph expressed as a DataFrame of edges through source vertex object, destination vertex object and edge weight (a small price (storing redundant objects) to pay for Spark to generate it all in parallel) into GraphX Graph representation.\nimport org.apache.spark.graphx._ import org.apache.spark.rdd.RDD case class Obj (id: String) // graph DataFrame Schema -- src: Struct, dst: Struct, weight: DoubleType val idx = graph.select($\u0026#34;src\u0026#34;.as(\u0026#34;obj\u0026#34;)) .union(graph.select($\u0026#34;dst\u0026#34;.\u0026#34;obj\u0026#34;)) .distinct .withColumn(\u0026#34;id\u0026#34;, monotonically_increasing_id()) .select(\u0026#34;id\u0026#34;, \u0026#34;obj\u0026#34;) .as[(VertexId, Obj)] .cache val vertices: RDD[(VertexId, Cell)] = idx.rdd val edges: RDD[Edge[Double]] = graph .select($\u0026#34;src.id\u0026#34;.as(\u0026#34;src\u0026#34;), $\u0026#34;dst.id\u0026#34;.as(\u0026#34;dst\u0026#34;), $\u0026#34;weight\u0026#34;.as(\u0026#34;attr\u0026#34;)) .join(idx.select($\u0026#34;id\u0026#34;.as(\u0026#34;srcId\u0026#34;), $\u0026#34;cell.id\u0026#34;.as(\u0026#34;src\u0026#34;)), \u0026#34;src\u0026#34;) .join(idx.select($\u0026#34;id\u0026#34;.as(\u0026#34;dstId\u0026#34;), $\u0026#34;cell.id\u0026#34;.as(\u0026#34;dst\u0026#34;)), \u0026#34;dst\u0026#34;) .select(\u0026#34;srcId\u0026#34;, \u0026#34;dstId\u0026#34;, \u0026#34;attr\u0026#34;) // GraphX\u0026#39;s required fields for an Edge object  .as[Edge[Double]].rdd.cache // caching edges is optional  val graph = Graph(vertices, edges) The case class Obj requires a simple unique id that doesn\u0026rsquo;t have to be a number (if so, it can be used as VertexId directly) as joining case classes are problematic with it. The idx DataFrame is cached for persistence and then assigned to the edges through join operations. It can also be referred to later for conversion of VertexId back to its corresponding object.\nClique Detection Problem Although GraphX supports popular graph algorithms such as PageRank, Connected Components and Strongly Connected Components, Clique Detection is missing from its features. It\u0026rsquo;s probably due to the non-parallel nature of the original Bron-Kerbosch algorithm.\nSince it\u0026rsquo;s the most suitable algorithm for my task on hand, research papers aren\u0026rsquo;t going to cut it. I found a simple Scala-based implementation, cleaned it up and optimized it for my use. By offloading heavy-lifting work to Spark as much as possible, this tiny recursive function (running on a single core) is surprisingly fast. My runtime for a graph with around 120,000 edges took 45 seconds. The graph is filtered for the important edges (based on the weight I defined) generated in the entirety of Singapore. So, to me, it is probably enough.\n\rThe function may not perform well in a densely-connected graph, but you should try it before calling it unusable.\nLarger graph, on the other hand, could be handled through first partitioning the graph based on certain criteria and run multiple instances of this function on difference partitions.\n","date":"2018-12-15T00:00:00Z","permalink":"https://chen-zhe.github.io/blog/p/basic-usage-of-graphx-for-clique-detection/","title":"Basic usage of GraphX for clique detection"},{"content":"Not long ago, I was tasked to improve our Spark application\u0026rsquo;s runtime performance as some modules takes 7 hours or more to complete on datasets that are not considered very large. This is my conclusion and afterthoughts after countless hours staring at our codebase and YARN application tracking UI. I was able to achieve 30% ~ 80% reduction of runtime, depending on how well the module was written and the nature of operations. But one thing is in common for the modules with maximum achieved runtime reduction, that is refactoring of join operations.\nRefactoring Joins join is a slow operation and everyone knows it, but sometime it’s just unavoidable. So, the real question is, how to prevent unnecessary use of joins. Interestingly, the examples that I discovered in the code base are all related to groupBy operations as well and gist of it can be summarized simply as: use a single groupBy to accomplish as much as possible. Here’s a few (Scala style) pseudo code blocks that I’ve seen drastic runtime improvements by removing the joins\n1. // Original df.filter(cond1) .groupBy(colA) .agg(sum(colB).as(colB)) .join(df.filter(cond2) .groupBy(colA) .agg(max(colB).as(colC)) , colA) // Optimized df.groupBy(colA) .agg(sum(when(cond1, colB) .otherwise(sth)) .as(colB), max(when(cond2, colB)) .as(colC)) Essentially, this code block is trying to get 2 different aggregated statistics of groups from colA based on certain conditions. The join operation then puts the 2 different statistics into the same table column-wise under the same group.\nIn this case, Spark’s (probably borrowed from SQL) When operator can be substituted for conditional filtering. Unless a default value or column is specified through the otherwise operator, when operator returns null. Spark’s built-in aggregation functions (max, sum, avg, etc.) will automatically ignore null values so when combined with when function, it acts as aggregations with conditions. The code snippet below is much more efficient, especially when more conditional aggregations are used.\n2. // Original res = df.groupBy(a,b) .agg(...as(c)) res.filter(b===1) .select(a, c.as(b1)) .join( res.filter(b===2) .select(a, c.as(b2)) , a) // Optimized df.groupBy(a) .pivot(b, Seq(1,2)) .agg(first(c)) .withColumnRenamed(...) Understanding what this piece of code is trying to do is important. In short, the code tries to group the table based on column A and then pivot on column B. Pivot operation can be considered as a groupBy on a certain column and transpose the result onto columns instead of rows.\nWith the understanding in mind, we can the proceed to rewrite this piece of code to use pivot operator? Wait, in this piece of code, we are only concerned with a value of 1 or 2 in column B. Worry not, Spark has it all prepared for you. Its pivot operation accepts an optional second parameter (a Sequence of values) and it will create the exact number of columns for every value in the parameter, regardless of the existence of the value in the pivoting column.\nSince we are sure that for each unique combination of values in Column A and Column B, there’s only a single row, we can use the first aggregation function.\n3. // Original impt = df.groupBy(a,b) .agg(...as(impt_val)) df.join(impt, Seq(a,b)) .filter(c \u0026gt; impt_val) .select(a,b,d) // Optimized df.withColumn(tmp, struct(c,d)) .groupBy(a,b) .agg(...as(impt_val), collect_list(tmp).as(tmp)) .withColumn(tmp, explode(tmp)) .filter(tmp.c \u0026gt; impt_val) .select(a,b,tmp.d) This piece of code first calculates an aggregated statistic per group and then use it as a threshold to filter the original dataframe. Optimizations to this example might be counter-intuitive, but in reality, if there are a lot of groups as defined by column A and column B, the join operation would take a significant amount of time.\nThe optimized version uses groupBy to get not only the threshold value, but also a list of values to be used in the filter and select statements later. All values are packed into a single compound column through struct operator and the aggregated list is unpacked through the explode operator, which flattens the list to each row and duplicates every other column. To some extent, the collect_list operation is not extremely efficient, but the overall execution time is still drastically faster.\nAfterthought Perhaps the main culprit here is the shuffling of partitions when executing groupBy and Join statements. As the shuffling might be different for one groupBy operation and one join operation, a third shuffle is required to bring rows of the same key into the same executor. In Spark, partition shuffle might be done over network and hence incurs significant overhead. In contrast, refactoring the operation into a single groupBy operator requires only 1 shuffle and hence runs much faster.\nOne (actually two) more thing While staring at the YARN application tracker UI, I have also discovered a few small steps that might impact performance significantly.\nDataframe df.count operation: count seems to be a harmless function but unlike its counterpart in Pandas, this operation requires a full execution of the SQL plan. In short, getting the count of rows in a Spark dataframe takes approximately the same amount of the time as generating the content of the entire dataframe.\nThus, when the dataframe is generated from complex operations, counting from saved parquet or cached content is faster.\nDataframe df.cache operation: caching a large dataset negatively impacts performance, which might stem from the disk I/O overhead due to spilling.\n","date":"2018-09-28T00:00:00Z","permalink":"https://chen-zhe.github.io/blog/p/apache-spark-save-time-with-less-join-operations/","title":"Apache Spark - Save Time with Less Join Operations"}]